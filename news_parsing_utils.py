import wmill
from typing import TypedDict, Dict, Any
import requests
import telegramify_markdown
import google.generativeai as genai
import json
import time
from google.api_core.exceptions import ResourceExhausted
import trafilatura
from bs4 import BeautifulSoup

genai.configure(api_key=wmill.get_variable("u/admin/googleai_api_key_free"))

def process_weather_info_with_gemini(data: Dict[str, Any], max_retries=3, delay_seconds=60):
    # (1) ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: ëª¨ë¸ì˜ ì—­í• , ê·œì¹™, í˜ë¥´ì†Œë‚˜ ì •ì˜
    SYSTEM_PROMPT = """
    ë‹¹ì‹ ì€ ë‚ ì”¨ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ì¡°ì–¸ì„ ì£¼ëŠ” ìœ ìš©í•œ AI ë¹„ì„œì…ë‹ˆë‹¤.
    ë‹¹ì‹ ì˜ ìœ ì¼í•œ ì„ë¬´ëŠ” ì…ë ¥ëœ JSON ë‚ ì”¨ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ë²ˆì—­ ë° ì™¸ì¶œ ì œì•ˆì´ í¬í•¨ëœ JSON ê°ì²´ í•˜ë‚˜ë¥¼ ë°˜í™˜í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
    ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ í…ìŠ¤íŠ¸ë¥¼ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.

    ë‹¤ìŒì€ 'suggestion' í•„ë“œë¥¼ ìƒì„±í•  ë•Œ ë°˜ë“œì‹œ ë”°ë¼ì•¼ í•  ê·œì¹™ì…ë‹ˆë‹¤ (ì´ ì™¸ì— ë‹¤ë¥¸ ì¡°ì–¸ì´ ìˆë‹¤ë©´ ì¶”ê°€í•´ë„ ì¢‹ìŠµë‹ˆë‹¤):
    - [ê°•ìˆ˜] 'ì˜¤ëŠ˜ ê°•ìˆ˜ í™•ë¥  (%)'ê°€ 30% ì´ìƒì´ë©´ ìš°ì‚°ì„ ì±™ê¸°ë¼ëŠ” ì¡°ì–¸ì„ í¬í•¨í•©ë‹ˆë‹¤.
    - [ëŒ€ê¸°ì§ˆ] 'ëŒ€ê¸°ì§ˆ ì§€ìˆ˜ (AQI)', 'ë¯¸ì„¸ë¨¼ì§€ (PM2.5)', 'ì´ˆë¯¸ì„¸ë¨¼ì§€ (PM10)', 'ì˜¤ì¡´ (O3)' ê°’ì— 'ë‚˜ì¨' ë˜ëŠ” 'ë§¤ìš° ë‚˜ì¨'ì´ í¬í•¨ë˜ë©´, ì™¸ì¶œì„ ìì œí•˜ê±°ë‚˜ ë§ˆìŠ¤í¬ ì°©ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.
    - [ìì™¸ì„ ] 'ì˜¤ëŠ˜ ìì™¸ì„  ì§€ìˆ˜ (UVI)'ê°€ 6 ì´ìƒì´ë©´(ë†’ìŒ), 8 ì´ìƒì´ë©´(ë§¤ìš° ë†’ìŒ) ìì™¸ì„  ì°¨ë‹¨ì œ, ëª¨ì, ì„ ê¸€ë¼ìŠ¤ ë“±ì„ ê¶Œì¥í•©ë‹ˆë‹¤.
    - [ì¼êµì°¨] 'ìµœê³  ê¸°ì˜¨ (Â°C)'ê³¼ 'ìµœì € ê¸°ì˜¨ (Â°C)'ì˜ ì°¨ì´ê°€ 10ë„ ì´ìƒì´ë©´ ê²‰ì˜·ì„ ì±™ê²¨ ì²´ì˜¨ ì¡°ì ˆì— ìœ ì˜í•˜ë¼ê³  ì¡°ì–¸í•©ë‹ˆë‹¤.
    - [ë°”ëŒ] 'ì˜¤ëŠ˜ í’ì† (m/s)'ì´ 7 m/s ì´ìƒì´ë©´ ë°”ëŒì´ ê°•í•˜ê²Œ ë¶„ë‹¤ëŠ” ì‚¬ì‹¤ì„ ì–¸ê¸‰í•©ë‹ˆë‹¤.
    - [ê¸ì •] ë‚ ì”¨ì™€ ê³µê¸° ì§ˆì´ ëª¨ë‘ ì¢‹ë‹¤ë©´(ì˜ˆ: ë§‘ìŒ, ê°•ìˆ˜í™•ë¥  ë‚®ìŒ, AQI ì¢‹ìŒ/ë³´í†µ), ì•¼ì™¸ í™œë™í•˜ê¸° ì¢‹ì€ ë‚ ì”¨ë¼ê³  ì–¸ê¸‰í•©ë‹ˆë‹¤.
    - [ì¢…í•©] ì´ ëª¨ë“  ì¡°ê±´ì„ ì¢…í•©í•˜ì—¬ í•˜ë‚˜ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨ìœ¼ë¡œ 'suggestion'ì„ ë§Œë“­ë‹ˆë‹¤.
    - [ê°•ì¡°] íŠ¹íˆ ì™¸ì¶œì‹œ ìŠì§€ ë§ì•„ì•¼ í•  ê²ƒ(ìš°ì‚°, ë§ˆìŠ¤í¬, ì™¸íˆ¬, ì„ í¬ë¦¼, ì™¸ì¶œ ìì œ ë“±)ì— ëŒ€í•œ í‚¤ì›Œë“œëŠ” â˜‚ï¸, ğŸ˜·, ğŸ§¥, â˜€ï¸, ğŸ  ë“±ì˜ ì ì ˆí•œ ì´ëª¨ì§€ë¥¼ ë¶™ì—¬ì„œ ê°•ì¡°í•´ì£¼ì„¸ìš”.
    """

    # (2) ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿: ì‹¤ì œ ë°ì´í„°ì™€ ì‘ì—… ì§€ì‹œ
    USER_PROMPT_TEMPLATE = """
    ë‹¤ìŒ JSON ë‚ ì”¨ ë°ì´í„°ë¥¼ ë¶„ì„í•´ ì£¼ì„¸ìš”.

    [ì…ë ¥ ë°ì´í„°]
    {input_data}

    [ì¶œë ¥ ìŠ¤í‚¤ë§ˆ]
    {{
    "location_ko": "ë²ˆì—­ëœ ìœ„ì¹˜ ('ìœ„ì¹˜' í•„ë“œ ë²ˆì—­)",
    "summary_ko": "ë²ˆì—­ëœ ìš”ì•½ ('ìš”ì•½' í•„ë“œ ë²ˆì—­)",
    "alert_ko": "ë²ˆì—­ëœ ê²½ë³´ ('ê²½ë³´' í•„ë“œ ë²ˆì—­, ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)",
    "suggestion": "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì˜ ëª¨ë“  ê·œì¹™ì— ë”°ë¼ ìƒì„±ëœ ì¢…í•© ì™¸ì¶œ ì œì•ˆ ë©˜íŠ¸"
    }}
    """

    # (3) ìƒì„± ì„¤ì •: Temperature ë° JSON ëª¨ë“œ ì„¤ì •
    GENERATION_CONFIG = genai.GenerationConfig(
        temperature=0.2,  # ì¼ê´€ëœ ë…¼ë¦¬ + ì•½ê°„ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥
        response_mime_type="application/json" # JSON ì¶œë ¥ ëª¨ë“œ ê°•ì œ
    )

    print("Gemini APIì— ë‚ ì”¨ ë¶„ì„ ìš”ì²­ ì¤‘...")
        # 1. ëª¨ë¸ ì´ˆê¸°í™” (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸, ìƒì„± ì„¤ì • ì ìš©)
    model = genai.GenerativeModel(
        model_name='gemini-2.5-flash',
        system_instruction=SYSTEM_PROMPT,
        generation_config=GENERATION_CONFIG
    )
    
    # 2. ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ì™„ì„±
    # (json.dumpsë¡œ ë°ì´í„°ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜)
    user_prompt = USER_PROMPT_TEMPLATE.format(
        input_data=json.dumps(data, indent=2, ensure_ascii=False)
    )
        
    current_try = 0
    while current_try <= max_retries:
        try:
            # Send the text to the model.
            # The model already knows the rules from the SYSTEM_PROMPT.
            response = model.generate_content(user_prompt)
            
            # The model, in JSON mode, should return a clean JSON string.
            # We parse it into a Python dictionary.
            result_json = json.loads(response.text)
            return result_json

        except ResourceExhausted as e:
            # This exception is thrown on HTTP 429 (Rate Limit / Token Limit)
            current_try += 1
            if current_try > max_retries:
                print(f"[Error] Max retries reached for input: {user_prompt[:50]}...")
                print(f"Last error: {e}")
                raise
            
            print(f"[Warning] Rate limit exceeded. Waiting for {delay_seconds} seconds... (Attempt {current_try}/{max_retries})")
            time.sleep(delay_seconds)
        
        except json.JSONDecodeError as e:
            # The model returned invalid JSON
            print(f"[Error] Failed to decode JSON from model response.")
            print(f"       Input text was: {user_prompt[:100]}...")
            print(f"       Model response was: {response.text}")
            raise e
        
        except Exception as e:
            # Catch other potential errors (e.g., connection issues)
            print(f"[Error] An unexpected error occurred: {e}")
            raise e

    raise RuntimeError("Unknown error from AI process")


def process_text_with_gemini(text_input, max_retries=3, delay_seconds=60):
    """
    Processes a single text string using the Gemini API.
    
    It follows the logic in SYSTEM_PROMPT and handles rate limiting.
    
    Args:
        text_input (str): The raw English text to process.
        max_retries (int): Max number of retries on rate limit errors.
        delay_seconds (int): Seconds to wait between retries.

    Returns:
        dict: A dictionary in the format {'english': '...', 'korean': '...'}
              or None if processing fails after retries.
    """

    # This system prompt contains all the logic you requested.
    # The model will follow these rules.
    SYSTEM_PROMPT = """
    You are a text processing expert. Your task is to process the given English text and return a JSON object.

    Follow these steps precisely:
    1.  First, clean the input text by removing all XML, HTML, and Markdown syntax (e.g., tags like <p>, <div>, and markers like **, #, [text](link)). Get the raw, plain text content.
    2.  Count the number of sentences in this *cleaned* plain text.
    3.  Apply logic based on the sentence count:
        -   **If 2 sentences or fewer:** The 'english' field in the JSON must be the original *cleaned* text, exactly as it is.
        -   **If 3 sentences or more:** The 'english' field in the JSON must be a concise, one-or-two-sentence summary of the *cleaned* text.
    4.  Translate the content of the 'english' field (whether it's the original text or the summary) into Korean. Put this translation in the 'korean' field.
    5.  Return *only* the final JSON object, with the exact schema: {"english": "...", "korean": "..."}.
        Do not include any other text, explanations, or markdown delimiters (like ```json).
    """

    # Configure the model to use the system prompt and JSON output mode
    model = genai.GenerativeModel(
        'gemini-2.5-flash',
        system_instruction=SYSTEM_PROMPT,
        generation_config={
            "response_mime_type": "application/json",
            "temperature": 0.0  # <-- Add this line for maximum predictability
        }
    )
    current_try = 0
    while current_try <= max_retries:
        try:
            # Send the text to the model.
            # The model already knows the rules from the SYSTEM_PROMPT.
            response = model.generate_content(text_input)
            
            # The model, in JSON mode, should return a clean JSON string.
            # We parse it into a Python dictionary.
            result_json = json.loads(response.text)
            return result_json

        except ResourceExhausted as e:
            # This exception is thrown on HTTP 429 (Rate Limit / Token Limit)
            current_try += 1
            if current_try > max_retries:
                print(f"[Error] Max retries reached for input: {text_input[:50]}...")
                print(f"Last error: {e}")
                raise
            
            print(f"[Warning] Rate limit exceeded. Waiting for {delay_seconds} seconds... (Attempt {current_try}/{max_retries})")
            time.sleep(delay_seconds)
        
        except json.JSONDecodeError as e:
            # The model returned invalid JSON
            print(f"[Error] Failed to decode JSON from model response.")
            print(f"       Input text was: {text_input[:100]}...")
            print(f"       Model response was: {response.text}")
            raise e
        
        except Exception as e:
            # Catch other potential errors (e.g., connection issues)
            print(f"[Error] An unexpected error occurred: {e}")
            raise e

    raise RuntimeError("Unknown error from AI process")

def split_string_by_lines(long_string: str, max_length: int = 4096) -> list[str]:
    """
    ê¸´ ë¬¸ìì—´ì„ max_length ë¯¸ë§Œì˜ ì²­í¬ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
    ë‹¨, ë¼ì¸ì˜ ì¤‘ê°„ì´ ì˜ë¦¬ì§€ ì•Šë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.

    Args:
        long_string (str): ë¶„í• í•  ì›ë³¸ ë¬¸ìì—´.
        max_length (int): ê° ì²­í¬ì˜ ìµœëŒ€ ê¸¸ì´ (ì´ ê¸¸ì´ ë¯¸ë§Œ).

    Returns:
        list[str]: ë¶„í• ëœ ë¬¸ìì—´ ì²­í¬ ë¦¬ìŠ¤íŠ¸.
        
    ì°¸ê³ :
    ë§Œì•½ í•œ ì¤„ ìì²´ê°€ max_lengthë³´ë‹¤ ê¸´ ê²½ìš°,
    "ë¼ì¸ ì¤‘ê°„ì„ ìë¥´ì§€ ì•ŠëŠ”ë‹¤"ëŠ” ê·œì¹™ì„ ìš°ì„ í•˜ì—¬ í•´ë‹¹ ë¼ì¸ í•˜ë‚˜ê°€
    í•˜ë‚˜ì˜ ì²­í¬ë¥¼ êµ¬ì„±í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ ê²½ìš° í•´ë‹¹ ì²­í¬ëŠ” max_lengthë¥¼ ì´ˆê³¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    
    # 1. ë¬¸ìì—´ì„ ë¼ì¸ë³„ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤. (ê°œí–‰ ë¬¸ìë¥¼ ë³´ì¡´í•©ë‹ˆë‹¤)
    lines = long_string.splitlines(keepends=True)
    
    chunks = []
    current_chunk = ""
    
    for line in lines:
        # 2. ë§Œì•½ í•œ ì¤„ ìì²´ê°€ max_lengthë³´ë‹¤ ê¸´ ê²½ìš° (ì˜ˆì™¸ ì¼€ì´ìŠ¤)
        #    "ë¼ì¸ì„ ìë¥´ì§€ ì•ŠëŠ”ë‹¤"ëŠ” ê·œì¹™ì„ ìš°ì„ í•©ë‹ˆë‹¤.
        if len(line) >= max_length:
            # í˜„ì¬ê¹Œì§€ ëˆ„ì ëœ ì²­í¬ê°€ ìˆë‹¤ë©´ ë¨¼ì € ì¶”ê°€í•©ë‹ˆë‹¤.
            if current_chunk:
                chunks.append(current_chunk)
            
            # ì´ ê¸´ ë¼ì¸ì„ ê·¸ ìì²´ë¡œ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.
            chunks.append(line)
            
            # í˜„ì¬ ì²­í¬ë¥¼ ë¦¬ì…‹í•˜ê³  ë‹¤ìŒ ë¼ì¸ìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.
            current_chunk = ""
            continue
            
        # 3. í˜„ì¬ ë¼ì¸ì„ ì¶”ê°€í–ˆì„ ë•Œ max_lengthë¥¼ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        if len(current_chunk) + len(line) >= max_length:
            # ì´ˆê³¼í•œë‹¤ë©´, í˜„ì¬ê¹Œì§€ì˜ ì²­í¬ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.
            if current_chunk: # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹ ê²½ìš°ì—ë§Œ ì¶”ê°€
                chunks.append(current_chunk)
            
            # ìƒˆ ì²­í¬ëŠ” í˜„ì¬ ë¼ì¸ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.
            current_chunk = line
        else:
            # 4. max_lengthë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šìœ¼ë©´, í˜„ì¬ ì²­í¬ì— ë¼ì¸ì„ ëˆ„ì í•©ë‹ˆë‹¤.
            current_chunk += line
            
    # 5. ë§ˆì§€ë§‰ì— current_chunkì— ë‚¨ì•„ìˆëŠ” ë¬¸ìì—´ì´ ìˆë‹¤ë©´ ì¶”ê°€í•©ë‹ˆë‹¤.
    if current_chunk:
        chunks.append(current_chunk)
        
    return chunks

class telegram(TypedDict):
    token: str

def send_to_telegram(message: str, chat_id: int = int(wmill.get_variable("u/admin/telegram_chat_id")), escaped: bool = False, token = wmill.get_resource("u/admin/telegram_token_resource")):
    telegram_url = f"https://api.telegram.org/bot{token['token']}/sendMessage"
    text = message
    if not escaped:
        text = telegramify_markdown.markdownify(message),
    payload = {
        "chat_id": chat_id,
        "text": text,
        "parse_mode":'MarkdownV2'
    }
    response = requests.post(telegram_url, data=payload)
    return response.json()

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'
}

def send_long_message_to_telegram(message: str, chat_id: int = int(wmill.get_variable("u/admin/telegram_chat_id")), token = wmill.get_resource("u/admin/telegram_token_resource")):
    splitted_msg = split_string_by_lines(message)
    for m in splitted_msg:
        send_to_telegram(m, chat_id, token=token)

def get_content_from_link(url):
    try:
        response = requests.get(
            url, 
            headers=HEADERS,  # ì¤€ë¹„ëœ í—¤ë” ì‚¬ìš©
            timeout=10        # 10ì´ˆ ì´ìƒ ê±¸ë¦¬ë©´ ì¤‘ë‹¨
        )
        if response.status_code != 200:
            print("Failed to get html")
            return None
        downloaded_html = response.text
        if downloaded_html is None:
            print("Empty html")
            return None
        full_text = trafilatura.extract(
            downloaded_html,
            output_format='txt',      # 'txt' (ê¸°ë³¸ê°’), 'json', 'xml' ë“±
            include_comments=False,   # ëŒ“ê¸€ ì œì™¸
            include_tables=False,     # í‘œ(í…Œì´ë¸”) ì œì™¸
            no_fallback=False         # ê¸°ë³¸ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ ë°©ë²• ì‹œë„
        )
        if not full_text:
            print("too short html")
            return None
        return full_text
    except Exception as e:
        print(e)
        return None

def remove_html_tags_bs4(html_string):
    """
    Removes HTML tags from a string using BeautifulSoup and extracts pure text.
    """
    soup = BeautifulSoup(html_string, 'html.parser')
    return soup.get_text()


def main(x: str):
    return send_long_message_to_telegram(x)
